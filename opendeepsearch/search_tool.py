
import dotenv
import sys
import os
current_dir = os.path.dirname(__file__)  
project_root = os.path.abspath(os.path.join(current_dir, '..'))
sys.path.append(project_root)
from langchain_ollama import ChatOllama
from langchain.prompts import ChatPromptTemplate
from opendeepsearch.context_building.process_build_context import ProcessBuildContext

dotenv.load_dotenv()

class OpenDeepSearchTool:
    name = "web_search"
    description = """Performs web search based on your query,
        processes retrieved content, then returns the final answer generated by an LLM."""
    inputs = {
        "query": {
            "type": "string",
            "description": "The search query to perform",
        },
    }
    output_type = "string"

    def __init__(self,
                 chunk_size: int = 1000,
                 overlap_sentences: int = 2,
                 embed_model_name: str = 'jinaai/jina-embeddings-v3',
                 serper_api_key: str = None,
                 top_k: int = 5,
                 llm_model: str = "openchat:7b-v3.5-1210-q4_K_M",
                 temperature: float = 0.3):
        # Initialize components
        self.builder = ProcessBuildContext(
            chunk_size=chunk_size,
            overlap_sentences=overlap_sentences,
            embed_model_name=embed_model_name,
            serper_api_key=serper_api_key
        )
        self.top_k = top_k
        self.llm_model = llm_model
        self.temperature = temperature

    def run(self, query: str) -> str:

        try:
            context = self.builder.build_context(query, top_k=self.top_k)
        except Exception as e:
            return f"Error during context building: {e}"
        return self.answer(query, context)

    def answer(self, user_question: str, context: str) -> str:
        try:
            chat_ollama = ChatOllama(model=self.llm_model, temperature=self.temperature)
            search_answer_prompt = """You are an AI-powered search agent that takes in a user`s search query, retrieves relevant search results, and provides an accurate and concise answer based on the provided context.
"""
            prompt = ChatPromptTemplate.from_messages([
                ("system", search_answer_prompt),
                ("system", "Information:\n"+ str(context)),
                ("human", user_question)
            ])
            prompt_values = {}

            chain = prompt | chat_ollama
            response = chain.invoke(prompt_values)
            return response.content

        except Exception as e:
            return f"Error during answer generation: {e}"


if __name__ == '__main__':
    api_key = os.getenv('SERPER_API_KEY')
    search_tool = OpenDeepSearchTool(
        serper_api_key=api_key,
        top_k=3,
        llm_model="openchat:7b-v3.5-1210-q4_K_M",
        temperature=0.2
    )
    question = "Current population of Japan and India"
    answer = search_tool.run(question)
    print(f"Question: {question}\nAnswer: {answer}")